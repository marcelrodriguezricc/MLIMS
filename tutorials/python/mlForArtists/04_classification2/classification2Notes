Course: https://www.kadenze.com/courses/machine-learning-for-musicians-and-artists/info

Lesson 4 - Classification II (Design Considerations)

1. What is a good classifier?

    - A classifier which is not complex enough to properly represent the data is "underfit" to the data.

    - A classifier with 100% training accuracy would get all predictions correct.

        - May introduce noise through over complication and consideration of outliers.

        - A classifier which overly represents outliers can be considered to be "overfit"

        - Usually better given a larger amount of training data.

    - A classifier can also more effectively represent basic pattern, but not have 100% accuracy.

        - This can reduce noise (imperfect feature representation) to learn an underlying concept.

        - Usually better given less data.

    - Occam's Razor as a Rule of Thumb

        - Only allow complexity when it's justified by the data.

        - Don't make the classifier complicated unless there's evidence it will be better than a simple one.

2. Nearest Neighbor

    -  Euclidean distance between two points in euclidean space, such as in physical space, can be measured as sqrt((x1 - x2)^2 + (y1 - y2)^2).
    
        - With three features. it would be sqrt((x1 - x2)^2 + (y1 - y2)^2 + (z1 - z2)^2).

    - However, the calculation of different features using the nearest neighbor formula may result in numbers of different magnitudes.

            - For example, an average heart rate is between 60 to 100 beats per minute, but the average loudness of human speech is between 0.25 and 5.0 RMS. When applying the euclidean distance formula, the heart rate will always be over represented in the sum.

            - Features should be normalized so they fall within a range of 0 - 1.

            - Some features may be irrelevant.

    - K-Nearest Neighbor

        - How many of the nearest neighbors, represented by the integer K, will be considered when choosing a classification for the input data?

        - Can reduce overfitting, and make decision boundaries less complex.

        - Increasing K also requires more computation.

        - The concepts observed in K-Nearest Neighbor examples can help us:

            - To reason about training examples in a low dimensional feature space, and extend these concepts to higher dimensions.

            - Features of different magnitudes and irrelevant features can hurt classification.
        
        - Benefits of KNN:
            
            - Low training time.

            - Capable of arbitrarily complex boundaries.

            - Changing k adjusts smoothness.
        
        - Drawbacks of KNN:

            - Can be slow if there are training examples.

            - Vulnerable to noise.

            - Vulnerable to irrelevant and redundant features.

            - Ineffective with high number of features.

3. Naive Bayes

    - Computes likelihood of the given feature being from each class.

        - Given the observed feature, what is the probability that it is from class 1?

            - Then class 2?

            - Etc. for all classes.

            - Which has the highest probability?

    - Combine this with consideration fo the relative proportions of the classes to compute a new probability.

        - Probability Theory:

            - Allows us to fit curves to population using example measurements (training data).

                - Data points inside of a bell curve tend more toward the average, less spread.

    - Bayes Rule: 

        - If we want to know the probability of event A happening...

            - Given that we know a related event B is happening.
        
        - We can calculate this by multiplying the probability that A is happening overall and the probability of B given that A is happening, divided by the probability of B overall.

            - The divisor is ignored in practice, as it applied for both classifications.

    - Naive:

        - If given more than one feature, fit a normal curve to each training example, and multiply them all together.

            - This is only applicable when you assume features are independent (which is a naive assumption).

    - Benefits of Naive Bayes

        - Fast training.

        - Fast running.

        - Can get get information on class probability given input as opposed to just choosing a best class.

        - Not as sensitive to outliers and errors, less likely to overfit.

4. Decision Stumps & Decision Trees

    - Threshold decided by potential split point.

        - Draw lines between each point. Compute possible training error based on number of points from same class between line and the closest example of the next class. The line with the lowest number possible training error is the threshold.

    - A bunch of stumps can be combined into a decision tree.

        - It asks the same threshold question at each node.

            - A stack of greater or less than statements.
            
            - End decision node which chooses classification is called a leaf.

            - Eg:

            if (x< 100){
                if (y < 80) {
                    if (y < 35) {
                        return 1;
                    } else{
                        return2;
                    }
                } else {
                    return 1;
                }
                else {
                    return 1;
                }
            }

    - Entropy / Diversity

        - If two groups can be classified with a perfect split (single branch of decision tree), and there's no noise between them, they're considered to be homogenous (low entropy). A leaf is put on each side.

        -  If a dataset is split by a threshold, and the representation of each class is half and half, then they're considered to be highly diverse (high entropy).

        - More effective nodes are that which split the example classifications effectively (lowest entropy).

        - Nodes are applied recursively until all leafs are classified.

            - Leafs that occur after the splitting of data by several nodes can be discarded in a process called Pruning to avoid noise.

    - Benefits of Decision Trees

        - Fast training.

        - Fast running.

        - Look at flow charts and reason about them, then implement in if-else blocks.

        - If a feature is irrelevant to the classification problem, the decision tree can ignore it.

        - You don't have to tune parameters, and pruning can be used to reduce noise.


    - Drawbacks

        - They're not suited to classify with more complexity.

5. AdaBoost

    - Combines multiple basic classification algorithms into a more complex classification algorithm.

        - Generates a first stump (training example), then computes for accuracy (weight).

            - Incorrectly classified example get increased weight.

        - The weighted stump then is passed to another stump, which computes a new threshold accounting for the weighted error.

        - This is iterated a set number of times, giving each iteration a score based on accuracy.

        - All of these iterations are combined to create a final classifier.

    - Works on more than two classes.

    - Available in Wekinator.

    - Benefits:

        - Fast to train.

        - Deals gracefully with irrelevant features.

        - Number of training rounds directly correlated to speed, so is easily optimizable.

        - Less prone to overfitting.

        - Easy to use.
        
        - Computing on new examples is fast.

    - Drawbacks:

        - Unable to draw complex curves to fit complex data.
    
6. Support Vector Machines

    - Draw boundaries in the decision space, with a priority toward simplicity.

    - Decision boundaries should maximize the margin, giving the most distance between examples and the threshold.

        - Width of the margin is distance from the decision boundary to the nearest point.

        - Tradeoff between margin distance and accuracy. 

    - Data is projected into a higher dimensional space.

        - Two dimensional space (x, y), is transferred to a three dimensional space (x, y, z).

        - A stump is then calculated in the higher dimensional space.

    - Able to perform linear, polynomial, RBF kernels.

        - Different levels of complexity.

    - Also has a C complexity constant.

    - Benefits:

        - Very powerful, works well on complicated problems.

        - Several kernel types to choose from.

        - Computationally inexpensive.

    - Drawbacks:

        - Slow for large datasets when optimizing.

        - Tough to use intuition to choose the correct kernel and parameters.

        - Do not work well with large datasets.

7. How to Evaluate Classifiers

    - Training accuracy balanced with noisy overfitting.

    - In Wekinator, you can do a Model Evaluation to find the accuracy. 

    - Cross validation

        - A measure of classifier "goodness" that assumes we want to most accurately classify new points in the future, assuming that those points are somewhat similar but not identical to training examples.

        - Split training data into several subsets (folds), for  training and testing.

            - Perform the test on each test set, and compute an accuracy score for each. Averaging them all together gives you a cross-validation accuracy score.

        - Used to compare algorithms on the same dataset.

    - Experimentation with the classifier trying different methods and parameters will always be necessary.

8. Probability Distribution

    - Wekinator can always give you a probability for each classification as opposed to outputting the class itself.

        - "Send class probabilities via OSC."

9. Multiple Classifiers at Once

    - Can be useful for comparison and interaction.

        - Two models can be trained on different datasets to look for different features and implemented simultaneously. 